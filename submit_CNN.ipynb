{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7045c7b8-85b8-44e9-bf79-bbbfe2477fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba34864-800d-4927-b78f-3d110adbb5f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36824d21-2f13-4af1-a45b-cfdd18ee78cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e893ef4a-2c2d-43e2-b0a7-1da56638f314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.init_W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.init_B(n_nodes2)\n",
    "        \n",
    "        # Adagradで使用する各層の前回までの重み\n",
    "        self.H_before_W = np.zeros_like(self.W)\n",
    "        self.H_before_B = np.zeros_like(self.B)\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"       \n",
    "        # 逆伝播で使用するためインスタンス化\n",
    "        self.X = X\n",
    "        \n",
    "        A = X @ self.W + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dB = dA.sum(axis=0)\n",
    "        self.dW = self.X.T @ dA\n",
    "        dZ = dA @ self.W.T\n",
    "        \n",
    "        # 更新\n",
    "        # ここの書き方がよくわかってない\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1626597c-8abe-4265-bd6a-e22a00736e24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SimpleInialzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66d0462d-ad2e-42b2-9f4d-3db845446bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        \n",
    "    def init_W(self, *shape):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 重みの初期値\n",
    "        \"\"\"\n",
    "        W = np.random.randn(*shape) * self.sigma\n",
    "        return W\n",
    "\n",
    "\n",
    "    def init_B(self, *shape):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :　バイアスの初期値\n",
    "        \"\"\"\n",
    "        B = np.zeros(*shape)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477f298-1045-4528-8f05-1138c5f0675f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2253b7f-2dce-43d6-b76a-5e12d0ff0c08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Wの更新\n",
    "        \n",
    "        param\n",
    "        --------------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        --------------\n",
    "        \n",
    "        return\n",
    "        ---------------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        # 引数のメンバ値を更新する\n",
    "        layer.W -= self.lr * layer.dW \n",
    "        layer.B -= self.lr * layer.dB \n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338879bb-f515-469f-a25b-d20525425fa0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f233ad-2a2f-437b-a372-98c5561790e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    sigmoid関数\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \"\"\"\n",
    "        Z = 1 / (1 + np.exp(-A))\n",
    "        \n",
    "        # 逆伝播でつかうためインスタンス化\n",
    "        self.Z = Z\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \"\"\"\n",
    "        dA = dZ * (self.Z * (1 - self.Z))\n",
    "        \n",
    "        return dA\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca942bef-09b0-4723-8116-4450e7f397de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "952fc8c5-7ab3-407a-ad11-8cd404a7930f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"\n",
    "    tanh関数\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \"\"\"\n",
    "        Z = np.tanh(A)\n",
    "        \n",
    "        # 逆伝播でつかうためインスタンス化\n",
    "        self.Z = Z\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \"\"\"\n",
    "        dA = dZ * (1 - self.Z ** 2)\n",
    "        \n",
    "        return dA\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc98127-8d0e-481a-8931-ee89761ab71f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "280c5da8-ce49-4f58-b23a-09a3334fec90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"\n",
    "    softmax関数\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \"\"\"\n",
    "        Z = np.exp(A) / np.exp(A).sum(axis=1).reshape(-1, 1)\n",
    "        \n",
    "        # 逆伝播でつかうためインスタンス化\n",
    "        self.Z = Z\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \"\"\"\n",
    "        # 交差エントロピー誤差層の逆伝播+softmax層の逆伝播\n",
    "        \n",
    "        dA = 1 / y.shape[0] * (self.Z - y)\n",
    "        \n",
    "        return dA\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3575f79f-1297-46dd-8f7a-39d15cfb982b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb32a3f5-4bdc-4d25-b0aa-da3f39aae17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    LeRU関数\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "        Z = np.maximum(0, A)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \"\"\"\n",
    "        dA = dZ * np.where(self.A > 0, 1, 0)\n",
    "        \n",
    "        return dA\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b82ed-9f4a-4cb5-92e1-7b4b6070c4d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## XavierInitializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b03c5d1-0708-445f-b62a-0606e0c95366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \n",
    "    \n",
    "    def init_W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 重みの初期値\n",
    "        \"\"\"\n",
    "        self.sigma = np.sqrt(1 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    \n",
    "    def init_B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :　バイアスの初期値\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86895028-8b7b-4b31-8157-e581cd18e789",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## HeInitializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4115c65d-3385-472c-879f-4032c270cc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "\n",
    "    def init_W(self, n_nodes1):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 重みの初期値\n",
    "        \"\"\"\n",
    "        self.sigma = math.sqrt(2 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "\n",
    "    def init_B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :　バイアスの初期値\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f77dee1-b788-478a-aec0-453c9e850749",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67c87477-9a24-4394-90b0-45f3576e7c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    各FC層の重み及びバイアスの更新\n",
    "    \n",
    "    param\n",
    "    -------------\n",
    "    lr : 学習率\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.delta = 1e-7\n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Wの更新\n",
    "        \n",
    "        param\n",
    "        --------------\n",
    "        layer : FCクラスのインスタンス\n",
    "        --------------\n",
    "        \n",
    "        return\n",
    "        ---------------\n",
    "        layer : FCクラスのインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        hW = layer.H_before_W + (layer.dW * layer.dW)\n",
    "        # h.shape = dw.shape = w.shape\n",
    "        \n",
    "        layer.W -= self.lr * layer.dW / np.sqrt(hW + self.delta)\n",
    "        # 割り算なのでdW / hは形が変わらないはず(掛け算であればアダマール積を取る。)\n",
    "        # そもそも行列割り算という概念がないため、割り算は同じ形じゃないとできない\n",
    "        \n",
    "        layer.H_before_W = hW\n",
    "        # 次使う場合のためにhは更新する\n",
    "                 \n",
    "        hB = layer.H_before_B + (layer.dB * layer.dB)\n",
    "        \n",
    "        layer.B -= self.lr * layer.dB / np.sqrt(hB + self.delta)\n",
    "        \n",
    "        layer.H_before_B = hB\n",
    "        # 次使う場合のためにhは更新する\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64202c9c-0cec-4b43-9ba0-bcab9b6487f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## getmnibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36f8e23d-0888-4d60-8304-19d0098f8a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        # ceilは切り上げ関数\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\n",
    "    \n",
    "    # len関数が使われるとこの値を返す。\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    # 要素をインスタンス変数に入れると値を返す\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "    \n",
    "    # iterが使われるとこの値を返す、なんでselfそのものを返す？？\n",
    "    # そういう物っぽい、nextと組み合わせて使われる\n",
    "    # for分繰り返されるという訳ではなく、forを行う前に一度だけ実行される\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    \n",
    "    # forの回数実行される\n",
    "    def __next__(self):\n",
    "        # 要素の終わりまでいったら自動的に終了する\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bd610a-f3cc-4a58-b2e7-e4a37a6f4dc4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DNN(Activete : ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb315a9-ef34-4359-b9c1-1e4a3fb99bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkClassifier:\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_nodes1, n_nodes2, n_output, epoch=20, lr=0.02, sigma=0.1, verbose = True):\n",
    "        \"\"\"\n",
    "        self.sigma : ガウス分布の標準偏差\n",
    "        self.lr : 学習率\n",
    "        self.n_nodes1 : 1層目のノード数\n",
    "        self.n_nodes2 : 2層目のノード数\n",
    "        self.n_output : 出力層のノード数\n",
    "        self.epoch : エポック数\n",
    "        self.loss_train : 訓練データの損失\n",
    "        self.loss_val : 検証データの損失\n",
    "        self.verbose : 学習過程を表示するか\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "        self.lr = lr\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.epoch = epoch\n",
    "        self.loss_train = np.zeros(epoch)\n",
    "        self.loss_val = np.zeros(epoch)\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        \n",
    "    def get_loss(self, X, y_ture):\n",
    "        \"\"\"\n",
    "        クロスエントロピー誤差を計算\n",
    "        log(X)が最大値0の値を取るため-をかける必要がある。\n",
    "        全て正解の場合0を取る。\n",
    "\n",
    "        param\n",
    "        -------------------\n",
    "        X : 次の形のndarray(batch_size, n_features)\n",
    "        入力値\n",
    "        y_ture : 次の形のndarray(batch_size, n_class)\n",
    "        正解ラベル\n",
    "\n",
    "        return\n",
    "        --------------------\n",
    "        L : float(スカラー)\n",
    "        \"\"\"\n",
    "\n",
    "        h = 1e-7      \n",
    "        L = - np.sum(y_ture * np.log(X + h) / len(y_ture))\n",
    "        return L\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "        訓練データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1層目の入力特徴量数\n",
    "        self.n_features = X.shape[1]\n",
    "        \n",
    "        # Affine, optimizer, initializerの決定\n",
    "        # Affine :3層, optimizer : SGD, initializer : simple\n",
    "        optimizer = SGD(self.lr)\n",
    "        self.Affine1 = Affine(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation1 = ReLU()\n",
    "        self.Affine2 = Affine(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation2 = ReLU()\n",
    "        self.Affine3 = Affine(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation3 = Softmax()      \n",
    "        \n",
    "        # エポック毎に更新\n",
    "        for i in range(self.epoch):\n",
    "            \n",
    "            # バッチ処理を実行\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=20)\n",
    "            \n",
    "            #\n",
    "            # バッチでループ\n",
    "            #\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "\n",
    "                # 順伝播\n",
    "                A1 = self.Conv1d.forward(X)\n",
    "                A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "\n",
    "                # 逆伝播\n",
    "                dA3 = self.activation3.backward(y_true) \n",
    "                dZ2 = self.FC3.backward(dA3)\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dA1 = dA1[:, np.newaxis]\n",
    "                dZ0 = self.Conv1d.backward(dA1) \n",
    "                \n",
    "            #\n",
    "            # 更新パラメータを使って全データで検証\n",
    "            #\n",
    "            A1 = self.Conv1d.forward(X)\n",
    "            A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "            Z1 = self.activation1.forward(A1)\n",
    "            A2 = self.FC2.forward(Z1)\n",
    "            Z2 = self.activation2.forward(A2)\n",
    "            A3 = self.FC3.forward(Z2)\n",
    "            Z3 = self.activation3.forward(A3)\n",
    "            \n",
    "            # lossの計算\n",
    "            self.loss_train[i] = self.get_loss(Z3, y)\n",
    "            \n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            if self.verbose:\n",
    "                print('loss : {}'.format(self.loss_train[i]))\n",
    "        \n",
    "            # 検証データ\n",
    "            if X_val is not None:\n",
    "                #\n",
    "                # 検証データの出力値を求める\n",
    "                #\n",
    "                A1 = self.Conv1d.forward(X)\n",
    "                A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                \n",
    "                # lossの計算\n",
    "                self.loss_val[i] = self.get_loss(Z3, y_val) \n",
    "            \n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        A1 = self.Conv1d.forward(X)\n",
    "        A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "\n",
    "        # 最も大きいインデックスをクラスとして採用\n",
    "        return np.argmax(Z3, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73124526-954c-4625-a489-a52adf46a25c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SimpleConv1d (問題1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a408537-2509-46d3-85ed-ef4919b2e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d():\n",
    "    \"\"\"\n",
    "    チャンネル数１の畳み込み層\n",
    "    \n",
    "    param\n",
    "    --------------------\n",
    "    self.W_size : ndarray (W_size, )\n",
    "    フィルタサイズ(重みのサイズ？？？？)\n",
    "    self.padding : int\n",
    "    パディングのサイズ\n",
    "    self.storide : int\n",
    "    ストライドのサイズ\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, W_size, optimizer, padding=0, storide=1):\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.padding = padding\n",
    "        self.storide = storide\n",
    "        \n",
    "        # Wの初期化(1次元のため、初期化クラスは使えない)\n",
    "        self.W = np.array([3, 5, 7], dtype=float)\n",
    "        #self.W = np.random.randn(W_size)\n",
    "        \n",
    "        # Bの初期化\n",
    "        self.B = np.array([1], dtype=float)\n",
    "        \n",
    "        \n",
    "    def get_N_out(self, N_in, padding=0, storide=1):\n",
    "        \"\"\"\n",
    "        畳み込みを行った後の特徴量の数\n",
    "        \"\"\"\n",
    "\n",
    "        # npではないのでpython組み込み関数を使用\n",
    "        N_out = int((N_in + 2 * self.padding - self.W.shape[0]) / self.storide + 1)\n",
    "\n",
    "        return N_out\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \n",
    "        param\n",
    "        --------------\n",
    "        self.X : ndarray (n_features, )\n",
    "        入力サイズ\n",
    "        \"\"\"\n",
    "        \n",
    "        # 逆伝播で使用する\n",
    "        self.X = X\n",
    "        \n",
    "        # 出力のサイズ指定\n",
    "        A = np.zeros(self.get_N_out(X.shape[0]))\n",
    "        \n",
    "        # aの値を更新\n",
    "        for i in range(A.shape[0]):\n",
    "            A[i] = int(X[i: i + self.W.shape[0]] @ self.W + self.B)\n",
    "\n",
    "        A = A.astype(int)\n",
    "        self.A = A\n",
    "        \n",
    "        return A\n",
    "        \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初期化(毎回行う)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        dB = np.zeros_like(self.B)\n",
    "\n",
    "        # 誤差の計算\n",
    "        for i in range(self.A.shape[0]):\n",
    "            dW = dW + dA[i] * self.X[i : i+self.W.shape[0]]\n",
    "            dB = dB + dA[i]\n",
    "        \n",
    "        \n",
    "        # 初期化\n",
    "        dX = np.zeros_like(self.X)\n",
    "\n",
    "        # 誤差の計算\n",
    "        for i in range(dX.shape[0]):\n",
    "            for s in range(self.W.shape[0]):\n",
    "                # 入力の枠外の微分は0とする(index,shapeの数え方が違うので-1)\n",
    "                if (i-s < 0) or (i-s > dA.shape[0]-1):\n",
    "                    pass\n",
    "                # 枠内の微分値は足し合わせる(重みが縦横反転する)\n",
    "                else:\n",
    "                    dX[i] += dA[i-s] * self.W[s]\n",
    "                    \n",
    "                    \n",
    "        # 更新\n",
    "        self.dW = dW\n",
    "        self.dB = dB\n",
    "        self = self.optimizer.update(self)\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5bb599-9b60-4ea9-b57a-72983ee11359",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "test = SimpleConv1d(W_size=3, optimizer=SGD(lr=0.01))\n",
    "test.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec3099-6378-40c2-8468-2773b8ea9c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_a = np.array([10, 20])\n",
    "test.backward(delta_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857bdfeb-d047-4157-80bc-01badc169b25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Conv1d (問題4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbce9cf3-ce06-483e-ae39-fb483379a503",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/EtJ2ZdWl.jpg)\n",
    "![Imgur](https://i.imgur.com/SICodEJl.jpg)\n",
    "![Imgur](https://i.imgur.com/gtU9vzZl.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a848c58-e899-4cb7-a248-ced656585a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d():\n",
    "    \"\"\"\n",
    "    チャンネル数１の畳み込み層\n",
    "    \n",
    "    param\n",
    "    --------------------\n",
    "    self.W_size : ndarray (W_size, )\n",
    "    フィルタサイズ(重みのサイズ？？？？)\n",
    "    self.padding : int\n",
    "    パディングのサイズ\n",
    "    self.storide : int\n",
    "    ストライドのサイズ\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_input, n_filiter, n_input_ch, n_output_ch, optimizer, initializer, padding=0, storide=1):\n",
    "        \n",
    "        self.n_input = n_input\n",
    "        self.n_filiter = n_filiter\n",
    "        self.optimizer = optimizer\n",
    "        self.n_input_ch = n_input_ch\n",
    "        self.n_output_ch = n_output_ch\n",
    "        self.W = initializer.init_W(n_output_ch, n_input_ch, n_filiter)\n",
    "        self.B = initializer.init_B(n_output_ch)\n",
    "        self.padding = padding\n",
    "        self.storide = storide\n",
    "        \n",
    "        # 出力サイズの計算\n",
    "        self.n_output = int((n_input + 2 * padding - n_filiter) / storide + 1)\n",
    "        \n",
    "        # Adagradで使用する各層の前回までの重み\n",
    "        self.H_before_W = np.zeros_like(self.W)\n",
    "        self.H_before_B = np.zeros_like(self.B)\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \n",
    "        param\n",
    "        --------------\n",
    "        self.X : ndarray (n_features, )\n",
    "        入力サイズ\n",
    "        \"\"\" \n",
    "        \n",
    "        \n",
    "#自作モデル、for文のため見送り\n",
    "\n",
    "\n",
    "#         # 形を作成(出力チャンネル、　出力ノード)\n",
    "#         A = np.zeros((3, 2))\n",
    "#         # Aの行を指定\n",
    "#         for i in range(A.shape[0]):\n",
    "#             # Aの列を指定\n",
    "#             for j in range(A.shape[1]):\n",
    "#                 # Xのチャンネルを指定\n",
    "#                 for k in range(X.shape[0]):\n",
    "#                     # Xの形をWが存在するところまでに切り取る(Wにゼロを追加するのと同義)\n",
    "#                     A[i, j] += X[k, j : j+w.shape[2]].T @ W[i, j]\n",
    "#                 # バイアスの計算\n",
    "#                 A[i, j] += b[i]\n",
    "\n",
    "#         A = A.astype(int)\n",
    "\n",
    "#         return A\n",
    "        \n",
    "        \n",
    "        # バッチ数\n",
    "        self.n_samples = X.shape[0]\n",
    "        \n",
    "        # 計算のために逆転させる\n",
    "        # 逆転させているとあるが、多分逆転してない\n",
    "        X = X.reshape(self.n_samples, self.n_input_ch, self.n_input)\n",
    "        \n",
    "        # Xの特徴量の左右に0埋め実施\n",
    "        # Aの形が(3, 2)のためXの特徴量方向に３パターンスライドさせてあげる必要がある。\n",
    "        # paddingが0であれば、そのまま０を使わないで解けるがpaddingがあると4, 6と増えていく。\n",
    "        self.X = np.pad(X, ((0,0), (0,0), ((self.n_filiter-1), 0)))\n",
    "        \n",
    "        # 出力配列（A）の計算のためゼロ配列X1を用意する\n",
    "        # サンプル毎にXをスライドさせたものをフィルタ数分用意する。\n",
    "        # 上で左右+1しているのでX+2の特徴量を持つ。\n",
    "        # Xをスライドさせることで、convを行っているイメージ\n",
    "        self.X1 = np.zeros((self.n_samples, self.n_input_ch, self.n_filiter, self.n_input+(self.n_filiter-1)))\n",
    "\n",
    "        # 重みの長さでループ\n",
    "        for i in range(self.n_filiter):\n",
    "            # ずらしながら上書き\n",
    "            self.X1[:, :, i] = np.roll(self.X, -i, axis=-1)\n",
    "            \n",
    "        # 重みとバイアスを考慮して計算\n",
    "        # バカむずい\n",
    "        # 一度5次元まで拡張して、３次元方向に足し合わせてる。\n",
    "        # 自分の描いたfor文のほうがわかるが、計算がこちらの方が楽\n",
    "        # 足し合わせる方向は、サンプル数とフィルタ数方向\n",
    "        A = np.sum(self.X1[:, np.newaxis, :, :, self.n_filiter-1-self.padding:self.n_input+self.padding:self.storide]*self.W[:, :, :, np.newaxis], axis=(2, 3)) + self.B.reshape(-1,1)\n",
    "        return A\n",
    "        \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \"\"\"\n",
    "        \n",
    "# 自作記念モデル\n",
    "#\n",
    "#         # 初期化(毎回行う)\n",
    "#         dW = np.zeros_like(self.W)\n",
    "#         dB = np.zeros_like(self.B)\n",
    "\n",
    "#         # 誤差の計算\n",
    "#         for i in range(dW.shape[0]):\n",
    "#             for j in range(dW.shape[1]):\n",
    "#                 for l in range(dA.shape[1]):\n",
    "#                     dW[i, j] += dA[i, l] * self.X[j, l : l+dW.shape[2]]\n",
    "#                     if j == 0:\n",
    "#                         dB[i] += dA[i, l]\n",
    "        \n",
    "        \n",
    "#         # 初期化\n",
    "#         dX = np.zeros_like(self.X)\n",
    "\n",
    "#         # 誤差の計算\n",
    "#         for i in range(dX.shape[0]):\n",
    "#             for j in range(dX.shape[1]):\n",
    "#                 for k in range(dA.shape[0]):\n",
    "#                     for s in range(self.W.shape[2]):\n",
    "#                         # 入力の枠外の微分は0とする\n",
    "#                         if (j-s < 0) or (j-s > dA.shape[1]-1):\n",
    "#                             pass\n",
    "#                         # 枠内の微分値は足し合わせる\n",
    "#                         else:\n",
    "#                             dX[i, j] += dA[k, j-s] * self.W[k, i, s]\n",
    "\n",
    "        # 重みの勾配\n",
    "        self.dW = np.sum(dA[:, :, np.newaxis, np.newaxis]*self.X1[:, np.newaxis, :, :, self.n_filiter-1-self.padding:self.n_input+self.padding:self.storide], axis=(0, -1))\n",
    "        # バイアスの勾配\n",
    "        self.dB = np.sum(dA, axis=(0, -1))\n",
    "        # 逆伝播の値計算のためにdAを変形\n",
    "        self.dA = np.pad(dA, ((0,0), (0,0), (0, (self.n_filiter-1))))\n",
    "        # 出力配列（dX）の計算のためゼロ配列dA1を用意する\n",
    "        self.dA1 = np.zeros((self.n_samples, self.n_output_ch, self.n_filiter, self.dA.shape[-1]))\n",
    "        # 重みの長さでループ\n",
    "        for i in range(self.n_filiter):\n",
    "            self.dA1[:, :, i] = np.roll(self.dA, i, axis=-1)\n",
    "        dX = np.sum(self.W[:, :, :, np.newaxis]*self.dA1[:, :, np.newaxis], axis=(1,3))\n",
    "\n",
    "                     \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42106f43-4d61-418f-9de6-b028b89b66cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00109616, -0.01927108,  0.00827651],\n",
       "        [ 0.00205352, -0.00880943,  0.00459448]],\n",
       "\n",
       "       [[ 0.00465457, -0.00263397,  0.01836861],\n",
       "        [ 0.00858373,  0.00207386, -0.01462086]],\n",
       "\n",
       "       [[ 0.01260558, -0.00184931, -0.0074794 ],\n",
       "        [ 0.00461731, -0.00387589, -0.01238062]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Conv1d(n_input=4, n_filiter=3, n_input_ch=2, n_output_ch=3, initializer=SimpleInitializer(sigma=0.01), optimizer=SGD(lr=0.01))\n",
    "test.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3516818d-620c-49d5-a499-ea4e8d245e62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.01655981, -0.02861966],\n",
       "        [ 0.01939807,  0.03582401],\n",
       "        [-0.06544679, -0.07380913]],\n",
       "\n",
       "       [[-0.01655981, -0.02861966],\n",
       "        [ 0.01939807,  0.03582401],\n",
       "        [-0.06544679, -0.07380913]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.forward(X=np.array([[[1, 2, 3, 4], [2, 3, 4, 5]],[[1, 2, 3, 4], [2, 3, 4, 5]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe53a39-667c-4290-afe1-7b872b5170fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ScratchCNNClassifier (問題5-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb8aa3ea-1c10-4d45-a095-8c242128dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchCNNClassifier:\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_nodes2, n_output, n_filiter, epoch=20, lr=0.02, sigma=0.1, verbose = True, Activater=Tanh, Optimizer=AdaGrad):\n",
    "        \"\"\"\n",
    "        self.sigma : ガウス分布の標準偏差\n",
    "        self.lr : 学習率\n",
    "        self.n_nodes2 : 2層目のノード数\n",
    "        self.n_output : 出力層のノード数\n",
    "        self.epoch : エポック数\n",
    "        self.loss_train : 訓練データの損失\n",
    "        self.loss_val : 検証データの損失\n",
    "        self.verbose : 学習過程を表示するか\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "        self.lr = lr\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.n_filiter = n_filiter\n",
    "        self.epoch = epoch\n",
    "        self.loss_train = np.zeros(epoch)\n",
    "        self.loss_val = np.zeros(epoch)\n",
    "        self.verbose = verbose\n",
    "        self.Activater = Activater\n",
    "        if Activater == Sigmoid or Activater == Tanh:\n",
    "            self.Initializer = XavierInitializer\n",
    "        elif Activater == ReLU:\n",
    "            self.Initializer = HeInitializer\n",
    "        self.Optimizer = Optimizer\n",
    "        \n",
    "        \n",
    "    def get_loss(self, X, y_ture):\n",
    "        \"\"\"\n",
    "        クロスエントロピー誤差を計算\n",
    "        log(X)が最大値0の値を取るため-をかける必要がある。\n",
    "        全て正解の場合0を取る。\n",
    "\n",
    "        param\n",
    "        -------------------\n",
    "        X : 次の形のndarray(batch_size, n_features)\n",
    "        入力値\n",
    "        y_ture : 次の形のndarray(batch_size, n_class)\n",
    "        正解ラベル\n",
    "\n",
    "        return\n",
    "        --------------------\n",
    "        L : float(スカラー)\n",
    "        \"\"\"\n",
    "\n",
    "        h = 1e-7      \n",
    "        L = - np.sum(y_ture * np.log(X + h) / len(y_ture))\n",
    "        return L\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "        訓練データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1層目の入力特徴量数\n",
    "        self.n_features = X.shape[1]\n",
    "        \n",
    "        #\n",
    "        # フィルターの数はハイパーパラメータ\n",
    "        #\n",
    "        \n",
    "        # 784→778 Convなので初期化はシンプルなやつ\n",
    "        self.Conv1d = Conv1d(n_input=self.n_features, n_filiter=self.n_filiter, n_input_ch=1, n_output_ch=1, initializer=SimpleInitializer(self.sigma), optimizer=self.Optimizer(self.lr))\n",
    "        self.activation1 = ReLU()\n",
    "        # 778→400\n",
    "        self.Affine2 = Affine(n_nodes1=self.Conv1d.n_output, n_nodes2=self.n_nodes2, initializer=self.Initializer(), optimizer=self.Optimizer(self.lr))\n",
    "        self.activation2 = ReLU()\n",
    "        # 400→10\n",
    "        self.Affine3 = Affine(n_nodes1=self.n_nodes2, n_nodes2=self.n_output, initializer=self.Initializer(), optimizer=self.Optimizer(self.lr))\n",
    "        self.activation3 = Softmax()      \n",
    "        \n",
    "        # エポック毎に更新\n",
    "        for i in range(self.epoch):\n",
    "            \n",
    "            # バッチ処理を実行\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=20)\n",
    "            \n",
    "            #\n",
    "            # バッチでループ\n",
    "            #\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "\n",
    "                # 順伝播\n",
    "                A1 = self.Conv1d.forward(mini_X_train)\n",
    "                A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.Affine2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.Affine3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "\n",
    "                # 逆伝播\n",
    "                dA3 = self.activation3.backward(mini_y_train) \n",
    "                dZ2 = self.Affine3.backward(dA3)\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.Affine2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dA1 = dA1[:, np.newaxis]\n",
    "                dZ0 = self.Conv1d.backward(dA1) \n",
    "                \n",
    "            #\n",
    "            # 更新パラメータを使って全データで検証\n",
    "            #\n",
    "            A1 = self.Conv1d.forward(X)\n",
    "            A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "            Z1 = self.activation1.forward(A1)\n",
    "            A2 = self.Affine2.forward(Z1)\n",
    "            Z2 = self.activation2.forward(A2)\n",
    "            A3 = self.Affine3.forward(Z2)\n",
    "            Z3 = self.activation3.forward(A3)\n",
    "            \n",
    "            # lossの計算\n",
    "            self.loss_train[i] = self.get_loss(Z3, y)\n",
    "            \n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            if self.verbose:\n",
    "                print('loss : {}'.format(self.loss_train[i]))\n",
    "        \n",
    "            # 検証データ\n",
    "            if X_val is not None:\n",
    "                #\n",
    "                # 検証データの出力値を求める\n",
    "                #\n",
    "                A1 = self.Conv1d.forward(X)\n",
    "                A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.Affine2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.Affine3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                \n",
    "                # lossの計算\n",
    "                self.loss_val[i] = self.get_loss(Z3, y_val) \n",
    "            \n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        A1 = self.Conv1d.forward(X)\n",
    "        A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.Affine2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.Affine3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "\n",
    "        # 最も大きいインデックスをクラスとして採用\n",
    "        return np.argmax(Z3, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f545510-df82-4a6e-8119-1885606190be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## データ呼び出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e393748-9b0b-411a-8aab-d36475b4af18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ll/cdp3xh194t7_9c631lm07yx00000gn/T/xpython_25385/797439057.py:9: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_train = X_train.astype(np.float)\n",
      "/var/folders/ll/cdp3xh194t7_9c631lm07yx00000gn/T/xpython_25385/797439057.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_test = X_test.astype(np.float)\n"
     ]
    }
   ],
   "source": [
    "# データ読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 画像データ→行データに\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# 正規化\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# onehotベクトル化\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "# 訓練データと評価データに\n",
    "X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004ae207-04c6-4a38-bb1d-ad760c3a17af",
   "metadata": {
    "tags": []
   },
   "source": [
    "## データテスト (問題8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "441bf189-7af7-4fcd-b48f-8938a3eb7d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.3474151445104263\n",
      "loss : 0.18587008337945998\n",
      "loss : 0.11415309160752884\n",
      "loss : 0.07001890594241297\n",
      "loss : 0.045121322570963664\n",
      "loss : 0.02996402590214832\n",
      "loss : 0.020527056716755886\n",
      "loss : 0.014933895472887551\n",
      "loss : 0.01145086405366445\n",
      "loss : 0.009143521155446557\n",
      "loss : 0.007572240212282073\n",
      "loss : 0.006428984475571742\n",
      "loss : 0.005561779615762482\n",
      "loss : 0.004869386932522326\n",
      "loss : 0.004321815374433001\n",
      "loss : 0.003871053907142638\n",
      "loss : 0.0034966105996420247\n",
      "loss : 0.0031794556754198918\n",
      "loss : 0.002913300005262992\n",
      "loss : 0.002681960378377793\n"
     ]
    }
   ],
   "source": [
    "test_15 = ScratchCNNClassifier(n_nodes2=200, n_output=10, n_filiter=15)\n",
    "test_15.fit(X_train_[:1000], y_train_[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "02e8e225-1437-4094-8a98-4062f36046ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8894"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = test_15.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0bb1cff1-fcbe-4ef4-a670-554d353419c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.3747987017143381\n",
      "loss : 0.22858483357411552\n",
      "loss : 0.15288131796792997\n",
      "loss : 0.10513262647960298\n",
      "loss : 0.07353838155662988\n",
      "loss : 0.05260812322199335\n",
      "loss : 0.03876642076114238\n",
      "loss : 0.028782460129756587\n",
      "loss : 0.022124131264524914\n",
      "loss : 0.017247272117804677\n",
      "loss : 0.013910697369034452\n",
      "loss : 0.011535443754460296\n",
      "loss : 0.009740150693157871\n",
      "loss : 0.008391364823601945\n",
      "loss : 0.007331700517399073\n",
      "loss : 0.006490631197494703\n",
      "loss : 0.005801263539624252\n",
      "loss : 0.005227795188655701\n",
      "loss : 0.004758387739044469\n",
      "loss : 0.004352409635734007\n"
     ]
    }
   ],
   "source": [
    "test_2 = ScratchCNNClassifier(n_nodes2=200, n_output=10, n_filiter=2)\n",
    "test_2.fit(X_train_[:1000], y_train_[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "aaf05f53-b425-4149-aa4d-86b61c867851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8975"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = test_2.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
