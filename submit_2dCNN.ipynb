{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7045c7b8-85b8-44e9-bf79-bbbfe2477fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba34864-800d-4927-b78f-3d110adbb5f4",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36824d21-2f13-4af1-a45b-cfdd18ee78cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e893ef4a-2c2d-43e2-b0a7-1da56638f314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.init_W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.init_B(n_nodes2)\n",
    "        \n",
    "        # Adagradで使用する各層の前回までの重み\n",
    "        self.H_before_W = np.zeros_like(self.W)\n",
    "        self.H_before_B = np.zeros_like(self.B)\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"       \n",
    "        # 逆伝播で使用するためインスタンス化\n",
    "        self.X = X\n",
    "        \n",
    "        A = X @ self.W + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dB = dA.sum(axis=0)\n",
    "        self.dW = self.X.T @ dA\n",
    "        dZ = dA @ self.W.T\n",
    "        \n",
    "        # 更新\n",
    "        # ここの書き方がよくわかってない\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1626597c-8abe-4265-bd6a-e22a00736e24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SimpleInialzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66d0462d-ad2e-42b2-9f4d-3db845446bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        \n",
    "    def init_W(self, *shape):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 重みの初期値\n",
    "        \"\"\"\n",
    "        W = np.random.randn(*shape) * self.sigma\n",
    "        return W\n",
    "\n",
    "\n",
    "    def init_B(self, *shape):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :　バイアスの初期値\n",
    "        \"\"\"\n",
    "        B = np.zeros(*shape)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477f298-1045-4528-8f05-1138c5f0675f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2253b7f-2dce-43d6-b76a-5e12d0ff0c08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Wの更新\n",
    "        \n",
    "        param\n",
    "        --------------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        --------------\n",
    "        \n",
    "        return\n",
    "        ---------------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        # 引数のメンバ値を更新する\n",
    "        layer.W -= self.lr * layer.dW \n",
    "        layer.B -= self.lr * layer.dB \n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338879bb-f515-469f-a25b-d20525425fa0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f233ad-2a2f-437b-a372-98c5561790e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    sigmoid関数\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \"\"\"\n",
    "        Z = 1 / (1 + np.exp(-A))\n",
    "        \n",
    "        # 逆伝播でつかうためインスタンス化\n",
    "        self.Z = Z\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \"\"\"\n",
    "        dA = dZ * (self.Z * (1 - self.Z))\n",
    "        \n",
    "        return dA\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca942bef-09b0-4723-8116-4450e7f397de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "952fc8c5-7ab3-407a-ad11-8cd404a7930f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"\n",
    "    tanh関数\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \"\"\"\n",
    "        Z = np.tanh(A)\n",
    "        \n",
    "        # 逆伝播でつかうためインスタンス化\n",
    "        self.Z = Z\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \"\"\"\n",
    "        dA = dZ * (1 - self.Z ** 2)\n",
    "        \n",
    "        return dA\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc98127-8d0e-481a-8931-ee89761ab71f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "280c5da8-ce49-4f58-b23a-09a3334fec90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"\n",
    "    softmax関数\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \"\"\"\n",
    "        Z = np.exp(A) / np.exp(A).sum(axis=1).reshape(-1, 1)\n",
    "        \n",
    "        # 逆伝播でつかうためインスタンス化\n",
    "        self.Z = Z\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \"\"\"\n",
    "        # 交差エントロピー誤差層の逆伝播+softmax層の逆伝播\n",
    "        \n",
    "        dA = 1 / y.shape[0] * (self.Z - y)\n",
    "        \n",
    "        return dA\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3575f79f-1297-46dd-8f7a-39d15cfb982b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb32a3f5-4bdc-4d25-b0aa-da3f39aae17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    LeRU関数\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "        Z = np.maximum(0, A)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \"\"\"\n",
    "        dA = dZ * np.where(self.A > 0, 1, 0)\n",
    "        \n",
    "        return dA\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b82ed-9f4a-4cb5-92e1-7b4b6070c4d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## XavierInitializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b03c5d1-0708-445f-b62a-0606e0c95366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \n",
    "    \n",
    "    def init_W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 重みの初期値\n",
    "        \"\"\"\n",
    "        self.sigma = np.sqrt(1 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    \n",
    "    def init_B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :　バイアスの初期値\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86895028-8b7b-4b31-8157-e581cd18e789",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## HeInitializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4115c65d-3385-472c-879f-4032c270cc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "\n",
    "    def init_W(self, n_nodes1):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 重みの初期値\n",
    "        \"\"\"\n",
    "        self.sigma = math.sqrt(2 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "\n",
    "    def init_B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :　バイアスの初期値\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f77dee1-b788-478a-aec0-453c9e850749",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c87477-9a24-4394-90b0-45f3576e7c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    各FC層の重み及びバイアスの更新\n",
    "    \n",
    "    param\n",
    "    -------------\n",
    "    lr : 学習率\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.delta = 1e-7\n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Wの更新\n",
    "        \n",
    "        param\n",
    "        --------------\n",
    "        layer : FCクラスのインスタンス\n",
    "        --------------\n",
    "        \n",
    "        return\n",
    "        ---------------\n",
    "        layer : FCクラスのインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        hW = layer.H_before_W + (layer.dW * layer.dW)\n",
    "        # h.shape = dw.shape = w.shape\n",
    "        \n",
    "        layer.W -= self.lr * layer.dW / np.sqrt(hW + self.delta)\n",
    "        # 割り算なのでdW / hは形が変わらないはず(掛け算であればアダマール積を取る。)\n",
    "        # そもそも行列割り算という概念がないため、割り算は同じ形じゃないとできない\n",
    "        \n",
    "        layer.H_before_W = hW\n",
    "        # 次使う場合のためにhは更新する\n",
    "                 \n",
    "        hB = layer.H_before_B + (layer.dB * layer.dB)\n",
    "        \n",
    "        layer.B -= self.lr * layer.dB / np.sqrt(hB + self.delta)\n",
    "        \n",
    "        layer.H_before_B = hB\n",
    "        # 次使う場合のためにhは更新する\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64202c9c-0cec-4b43-9ba0-bcab9b6487f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## getmnibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f8e23d-0888-4d60-8304-19d0098f8a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        # ceilは切り上げ関数\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\n",
    "    \n",
    "    # len関数が使われるとこの値を返す。\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    # 要素をインスタンス変数に入れると値を返す\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "    \n",
    "    # iterが使われるとこの値を返す、なんでselfそのものを返す？？\n",
    "    # そういう物っぽい、nextと組み合わせて使われる\n",
    "    # for分繰り返されるという訳ではなく、forを行う前に一度だけ実行される\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    \n",
    "    # forの回数実行される\n",
    "    def __next__(self):\n",
    "        # 要素の終わりまでいったら自動的に終了する\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bd610a-f3cc-4a58-b2e7-e4a37a6f4dc4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DNN(Activete : ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb315a9-ef34-4359-b9c1-1e4a3fb99bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkClassifier:\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_nodes1, n_nodes2, n_output, epoch=20, lr=0.02, sigma=0.1, verbose = True):\n",
    "        \"\"\"\n",
    "        self.sigma : ガウス分布の標準偏差\n",
    "        self.lr : 学習率\n",
    "        self.n_nodes1 : 1層目のノード数\n",
    "        self.n_nodes2 : 2層目のノード数\n",
    "        self.n_output : 出力層のノード数\n",
    "        self.epoch : エポック数\n",
    "        self.loss_train : 訓練データの損失\n",
    "        self.loss_val : 検証データの損失\n",
    "        self.verbose : 学習過程を表示するか\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "        self.lr = lr\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.epoch = epoch\n",
    "        self.loss_train = np.zeros(epoch)\n",
    "        self.loss_val = np.zeros(epoch)\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        \n",
    "    def get_loss(self, X, y_ture):\n",
    "        \"\"\"\n",
    "        クロスエントロピー誤差を計算\n",
    "        log(X)が最大値0の値を取るため-をかける必要がある。\n",
    "        全て正解の場合0を取る。\n",
    "\n",
    "        param\n",
    "        -------------------\n",
    "        X : 次の形のndarray(batch_size, n_features)\n",
    "        入力値\n",
    "        y_ture : 次の形のndarray(batch_size, n_class)\n",
    "        正解ラベル\n",
    "\n",
    "        return\n",
    "        --------------------\n",
    "        L : float(スカラー)\n",
    "        \"\"\"\n",
    "\n",
    "        h = 1e-7      \n",
    "        L = - np.sum(y_ture * np.log(X + h) / len(y_ture))\n",
    "        return L\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "        訓練データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1層目の入力特徴量数\n",
    "        self.n_features = X.shape[1]\n",
    "        \n",
    "        # Affine, optimizer, initializerの決定\n",
    "        # Affine :3層, optimizer : SGD, initializer : simple\n",
    "        optimizer = SGD(self.lr)\n",
    "        self.Affine1 = Affine(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation1 = ReLU()\n",
    "        self.Affine2 = Affine(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation2 = ReLU()\n",
    "        self.Affine3 = Affine(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation3 = Softmax()      \n",
    "        \n",
    "        # エポック毎に更新\n",
    "        for i in range(self.epoch):\n",
    "            \n",
    "            # バッチ処理を実行\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=20)\n",
    "            \n",
    "            #\n",
    "            # バッチでループ\n",
    "            #\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "\n",
    "                # 順伝播\n",
    "                A1 = self.Conv1d.forward(X)\n",
    "                A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "\n",
    "                # 逆伝播\n",
    "                dA3 = self.activation3.backward(y_true) \n",
    "                dZ2 = self.FC3.backward(dA3)\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dA1 = dA1[:, np.newaxis]\n",
    "                dZ0 = self.Conv1d.backward(dA1) \n",
    "                \n",
    "            #\n",
    "            # 更新パラメータを使って全データで検証\n",
    "            #\n",
    "            A1 = self.Conv1d.forward(X)\n",
    "            A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "            Z1 = self.activation1.forward(A1)\n",
    "            A2 = self.FC2.forward(Z1)\n",
    "            Z2 = self.activation2.forward(A2)\n",
    "            A3 = self.FC3.forward(Z2)\n",
    "            Z3 = self.activation3.forward(A3)\n",
    "            \n",
    "            # lossの計算\n",
    "            self.loss_train[i] = self.get_loss(Z3, y)\n",
    "            \n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            if self.verbose:\n",
    "                print('loss : {}'.format(self.loss_train[i]))\n",
    "        \n",
    "            # 検証データ\n",
    "            if X_val is not None:\n",
    "                #\n",
    "                # 検証データの出力値を求める\n",
    "                #\n",
    "                A1 = self.Conv1d.forward(X)\n",
    "                A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                \n",
    "                # lossの計算\n",
    "                self.loss_val[i] = self.get_loss(Z3, y_val) \n",
    "            \n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        A1 = self.Conv1d.forward(X)\n",
    "        A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "\n",
    "        # 最も大きいインデックスをクラスとして採用\n",
    "        return np.argmax(Z3, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73124526-954c-4625-a489-a52adf46a25c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SimpleConv1d (問題1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a408537-2509-46d3-85ed-ef4919b2e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d():\n",
    "    \"\"\"\n",
    "    チャンネル数１の畳み込み層\n",
    "    \n",
    "    param\n",
    "    --------------------\n",
    "    self.W_size : ndarray (W_size, )\n",
    "    フィルタサイズ(重みのサイズ？？？？)\n",
    "    self.padding : int\n",
    "    パディングのサイズ\n",
    "    self.storide : int\n",
    "    ストライドのサイズ\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, W_size, optimizer, padding=0, storide=1):\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.padding = padding\n",
    "        self.storide = storide\n",
    "        \n",
    "        # Wの初期化(1次元のため、初期化クラスは使えない)\n",
    "        self.W = np.array([3, 5, 7], dtype=float)\n",
    "        #self.W = np.random.randn(W_size)\n",
    "        \n",
    "        # Bの初期化\n",
    "        self.B = np.array([1], dtype=float)\n",
    "        \n",
    "        \n",
    "    def get_N_out(self, N_in, padding=0, storide=1):\n",
    "        \"\"\"\n",
    "        畳み込みを行った後の特徴量の数\n",
    "        \"\"\"\n",
    "\n",
    "        # npではないのでpython組み込み関数を使用\n",
    "        N_out = int((N_in + 2 * self.padding - self.W.shape[0]) / self.storide + 1)\n",
    "\n",
    "        return N_out\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \n",
    "        param\n",
    "        --------------\n",
    "        self.X : ndarray (n_features, )\n",
    "        入力サイズ\n",
    "        \"\"\"\n",
    "        \n",
    "        # 逆伝播で使用する\n",
    "        self.X = X\n",
    "        \n",
    "        # 出力のサイズ指定\n",
    "        A = np.zeros(self.get_N_out(X.shape[0]))\n",
    "        \n",
    "        # aの値を更新\n",
    "        for i in range(A.shape[0]):\n",
    "            A[i] = int(X[i: i + self.W.shape[0]] @ self.W + self.B)\n",
    "\n",
    "        A = A.astype(int)\n",
    "        self.A = A\n",
    "        \n",
    "        return A\n",
    "        \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初期化(毎回行う)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        dB = np.zeros_like(self.B)\n",
    "\n",
    "        # 誤差の計算\n",
    "        for i in range(self.A.shape[0]):\n",
    "            dW = dW + dA[i] * self.X[i : i+self.W.shape[0]]\n",
    "            dB = dB + dA[i]\n",
    "        \n",
    "        \n",
    "        # 初期化\n",
    "        dX = np.zeros_like(self.X)\n",
    "\n",
    "        # 誤差の計算\n",
    "        for i in range(dX.shape[0]):\n",
    "            for s in range(self.W.shape[0]):\n",
    "                # 入力の枠外の微分は0とする(index,shapeの数え方が違うので-1)\n",
    "                if (i-s < 0) or (i-s > dA.shape[0]-1):\n",
    "                    pass\n",
    "                # 枠内の微分値は足し合わせる(重みが縦横反転する)\n",
    "                else:\n",
    "                    dX[i] += dA[i-s] * self.W[s]\n",
    "                    \n",
    "                    \n",
    "        # 更新\n",
    "        self.dW = dW\n",
    "        self.dB = dB\n",
    "        self = self.optimizer.update(self)\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5bb599-9b60-4ea9-b57a-72983ee11359",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "test = SimpleConv1d(W_size=3, optimizer=SGD(lr=0.01))\n",
    "test.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec3099-6378-40c2-8468-2773b8ea9c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_a = np.array([10, 20])\n",
    "test.backward(delta_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857bdfeb-d047-4157-80bc-01badc169b25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Conv1d (問題4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbce9cf3-ce06-483e-ae39-fb483379a503",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/EtJ2ZdWl.jpg)\n",
    "![Imgur](https://i.imgur.com/SICodEJl.jpg)\n",
    "![Imgur](https://i.imgur.com/gtU9vzZl.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a848c58-e899-4cb7-a248-ced656585a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d():\n",
    "    \"\"\"\n",
    "    チャンネル数１の畳み込み層\n",
    "    \n",
    "    param\n",
    "    --------------------\n",
    "    self.W_size : ndarray (W_size, )\n",
    "    フィルタサイズ(重みのサイズ？？？？)\n",
    "    self.padding : int\n",
    "    パディングのサイズ\n",
    "    self.storide : int\n",
    "    ストライドのサイズ\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_input, n_filiter, n_input_ch, n_output_ch, optimizer, initializer, padding=0, storide=1):\n",
    "        \n",
    "        self.n_input = n_input\n",
    "        self.n_filiter = n_filiter\n",
    "        self.optimizer = optimizer\n",
    "        self.n_input_ch = n_input_ch\n",
    "        self.n_output_ch = n_output_ch\n",
    "        self.W = initializer.init_W(n_output_ch, n_input_ch, n_filiter)\n",
    "        self.B = initializer.init_B(n_output_ch)\n",
    "        self.padding = padding\n",
    "        self.storide = storide\n",
    "        \n",
    "        # 出力サイズの計算\n",
    "        self.n_output = int((n_input + 2 * padding - n_filiter) / storide + 1)\n",
    "        \n",
    "        # Adagradで使用する各層の前回までの重み\n",
    "        self.H_before_W = np.zeros_like(self.W)\n",
    "        self.H_before_B = np.zeros_like(self.B)\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \n",
    "        param\n",
    "        --------------\n",
    "        self.X : ndarray (n_features, )\n",
    "        入力サイズ\n",
    "        \"\"\" \n",
    "        \n",
    "        \n",
    "#自作モデル、for文のため見送り\n",
    "\n",
    "\n",
    "#         # 形を作成(出力チャンネル、　出力ノード)\n",
    "#         A = np.zeros((3, 2))\n",
    "#         # Aの行を指定\n",
    "#         for i in range(A.shape[0]):\n",
    "#             # Aの列を指定\n",
    "#             for j in range(A.shape[1]):\n",
    "#                 # Xのチャンネルを指定\n",
    "#                 for k in range(X.shape[0]):\n",
    "#                     # Xの形をWが存在するところまでに切り取る(Wにゼロを追加するのと同義)\n",
    "#                     A[i, j] += X[k, j : j+w.shape[2]].T @ W[i, j]\n",
    "#                 # バイアスの計算\n",
    "#                 A[i, j] += b[i]\n",
    "\n",
    "#         A = A.astype(int)\n",
    "\n",
    "#         return A\n",
    "        \n",
    "        \n",
    "        # バッチ数\n",
    "        self.n_samples = X.shape[0]\n",
    "        \n",
    "        # 計算のために逆転させる\n",
    "        # 逆転させているとあるが、多分逆転してない\n",
    "        X = X.reshape(self.n_samples, self.n_input_ch, self.n_input)\n",
    "        \n",
    "        # Xの特徴量の左右に0埋め実施\n",
    "        # Aの形が(3, 2)のためXの特徴量方向に３パターンスライドさせてあげる必要がある。\n",
    "        # paddingが0であれば、そのまま０を使わないで解けるがpaddingがあると4, 6と増えていく。\n",
    "        self.X = np.pad(X, ((0,0), (0,0), ((self.n_filiter-1), 0)))\n",
    "        \n",
    "        # 出力配列（A）の計算のためゼロ配列X1を用意する\n",
    "        # サンプル毎にXをスライドさせたものをフィルタ数分用意する。\n",
    "        # 上で左右+1しているのでX+2の特徴量を持つ。\n",
    "        # Xをスライドさせることで、convを行っているイメージ\n",
    "        self.X1 = np.zeros((self.n_samples, self.n_input_ch, self.n_filiter, self.n_input+(self.n_filiter-1)))\n",
    "\n",
    "        # 重みの長さでループ\n",
    "        for i in range(self.n_filiter):\n",
    "            # ずらしながら上書き\n",
    "            self.X1[:, :, i] = np.roll(self.X, -i, axis=-1)\n",
    "            \n",
    "        # 重みとバイアスを考慮して計算\n",
    "        # バカむずい\n",
    "        # 一度5次元まで拡張して、３次元方向に足し合わせてる。\n",
    "        # 自分の描いたfor文のほうがわかるが、計算がこちらの方が楽\n",
    "        # 足し合わせる方向は、サンプル数とフィルタ数方向\n",
    "        A = np.sum(self.X1[:, np.newaxis, :, :, self.n_filiter-1-self.padding:self.n_input+self.padding:self.storide]*self.W[:, :, :, np.newaxis], axis=(2, 3)) + self.B.reshape(-1,1)\n",
    "        return A\n",
    "        \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \"\"\"\n",
    "        \n",
    "# 自作記念モデル\n",
    "#\n",
    "#         # 初期化(毎回行う)\n",
    "#         dW = np.zeros_like(self.W)\n",
    "#         dB = np.zeros_like(self.B)\n",
    "\n",
    "#         # 誤差の計算\n",
    "#         for i in range(dW.shape[0]):\n",
    "#             for j in range(dW.shape[1]):\n",
    "#                 for l in range(dA.shape[1]):\n",
    "#                     dW[i, j] += dA[i, l] * self.X[j, l : l+dW.shape[2]]\n",
    "#                     if j == 0:\n",
    "#                         dB[i] += dA[i, l]\n",
    "        \n",
    "        \n",
    "#         # 初期化\n",
    "#         dX = np.zeros_like(self.X)\n",
    "\n",
    "#         # 誤差の計算\n",
    "#         for i in range(dX.shape[0]):\n",
    "#             for j in range(dX.shape[1]):\n",
    "#                 for k in range(dA.shape[0]):\n",
    "#                     for s in range(self.W.shape[2]):\n",
    "#                         # 入力の枠外の微分は0とする\n",
    "#                         if (j-s < 0) or (j-s > dA.shape[1]-1):\n",
    "#                             pass\n",
    "#                         # 枠内の微分値は足し合わせる\n",
    "#                         else:\n",
    "#                             dX[i, j] += dA[k, j-s] * self.W[k, i, s]\n",
    "\n",
    "        # 重みの勾配\n",
    "        self.dW = np.sum(dA[:, :, np.newaxis, np.newaxis]*self.X1[:, np.newaxis, :, :, self.n_filiter-1-self.padding:self.n_input+self.padding:self.storide], axis=(0, -1))\n",
    "        # バイアスの勾配\n",
    "        self.dB = np.sum(dA, axis=(0, -1))\n",
    "        # 逆伝播の値計算のためにdAを変形\n",
    "        self.dA = np.pad(dA, ((0,0), (0,0), (0, (self.n_filiter-1))))\n",
    "        # 出力配列（dX）の計算のためゼロ配列dA1を用意する\n",
    "        self.dA1 = np.zeros((self.n_samples, self.n_output_ch, self.n_filiter, self.dA.shape[-1]))\n",
    "        # 重みの長さでループ\n",
    "        for i in range(self.n_filiter):\n",
    "            self.dA1[:, :, i] = np.roll(self.dA, i, axis=-1)\n",
    "        dX = np.sum(self.W[:, :, :, np.newaxis]*self.dA1[:, :, np.newaxis], axis=(1,3))\n",
    "\n",
    "                     \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42106f43-4d61-418f-9de6-b028b89b66cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = Conv1d(n_input=4, n_filiter=3, n_input_ch=2, n_output_ch=3, initializer=SimpleInitializer(sigma=0.01), optimizer=SGD(lr=0.01))\n",
    "test.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516818d-620c-49d5-a499-ea4e8d245e62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.forward(X=np.array([[[1, 2, 3, 4], [2, 3, 4, 5]],[[1, 2, 3, 4], [2, 3, 4, 5]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe53a39-667c-4290-afe1-7b872b5170fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ScratchCNNClassifier 1d (問題5-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8aa3ea-1c10-4d45-a095-8c242128dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchCNNClassifier:\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_nodes2, n_output, n_filiter, epoch=20, lr=0.02, sigma=0.1, verbose = True, Activater=Tanh, Optimizer=AdaGrad):\n",
    "        \"\"\"\n",
    "        self.sigma : ガウス分布の標準偏差\n",
    "        self.lr : 学習率\n",
    "        self.n_nodes2 : 2層目のノード数\n",
    "        self.n_output : 出力層のノード数\n",
    "        self.epoch : エポック数\n",
    "        self.loss_train : 訓練データの損失\n",
    "        self.loss_val : 検証データの損失\n",
    "        self.verbose : 学習過程を表示するか\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "        self.lr = lr\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.n_filiter = n_filiter\n",
    "        self.epoch = epoch\n",
    "        self.loss_train = np.zeros(epoch)\n",
    "        self.loss_val = np.zeros(epoch)\n",
    "        self.verbose = verbose\n",
    "        self.Activater = Activater\n",
    "        if Activater == Sigmoid or Activater == Tanh:\n",
    "            self.Initializer = XavierInitializer\n",
    "        elif Activater == ReLU:\n",
    "            self.Initializer = HeInitializer\n",
    "        self.Optimizer = Optimizer\n",
    "        \n",
    "        \n",
    "    def get_loss(self, X, y_ture):\n",
    "        \"\"\"\n",
    "        クロスエントロピー誤差を計算\n",
    "        log(X)が最大値0の値を取るため-をかける必要がある。\n",
    "        全て正解の場合0を取る。\n",
    "\n",
    "        param\n",
    "        -------------------\n",
    "        X : 次の形のndarray(batch_size, n_features)\n",
    "        入力値\n",
    "        y_ture : 次の形のndarray(batch_size, n_class)\n",
    "        正解ラベル\n",
    "\n",
    "        return\n",
    "        --------------------\n",
    "        L : float(スカラー)\n",
    "        \"\"\"\n",
    "\n",
    "        h = 1e-7      \n",
    "        L = - np.sum(y_ture * np.log(X + h) / len(y_ture))\n",
    "        return L\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "        訓練データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1層目の入力特徴量数\n",
    "        self.n_features = X.shape[1]\n",
    "        \n",
    "        #\n",
    "        # フィルターの数はハイパーパラメータ\n",
    "        #\n",
    "        \n",
    "        # 784→778 Convなので初期化はシンプルなやつ\n",
    "        self.Conv1d = Conv1d(n_input=self.n_features, n_filiter=self.n_filiter, n_input_ch=1, n_output_ch=1, initializer=SimpleInitializer(self.sigma), optimizer=self.Optimizer(self.lr))\n",
    "        self.activation1 = ReLU()\n",
    "        # 778→400\n",
    "        self.Affine2 = Affine(n_nodes1=self.Conv1d.n_output, n_nodes2=self.n_nodes2, initializer=self.Initializer(), optimizer=self.Optimizer(self.lr))\n",
    "        self.activation2 = ReLU()\n",
    "        # 400→10\n",
    "        self.Affine3 = Affine(n_nodes1=self.n_nodes2, n_nodes2=self.n_output, initializer=self.Initializer(), optimizer=self.Optimizer(self.lr))\n",
    "        self.activation3 = Softmax()      \n",
    "        \n",
    "        # エポック毎に更新\n",
    "        for i in range(self.epoch):\n",
    "            \n",
    "            # バッチ処理を実行\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=20)\n",
    "            \n",
    "            #\n",
    "            # バッチでループ\n",
    "            #\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "\n",
    "                # 順伝播\n",
    "                A1 = self.Conv1d.forward(mini_X_train)\n",
    "                A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.Affine2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.Affine3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "\n",
    "                # 逆伝播\n",
    "                dA3 = self.activation3.backward(mini_y_train) \n",
    "                dZ2 = self.Affine3.backward(dA3)\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.Affine2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dA1 = dA1[:, np.newaxis]\n",
    "                dZ0 = self.Conv1d.backward(dA1) \n",
    "                \n",
    "            #\n",
    "            # 更新パラメータを使って全データで検証\n",
    "            #\n",
    "            A1 = self.Conv1d.forward(X)\n",
    "            A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "            Z1 = self.activation1.forward(A1)\n",
    "            A2 = self.Affine2.forward(Z1)\n",
    "            Z2 = self.activation2.forward(A2)\n",
    "            A3 = self.Affine3.forward(Z2)\n",
    "            Z3 = self.activation3.forward(A3)\n",
    "            \n",
    "            # lossの計算\n",
    "            self.loss_train[i] = self.get_loss(Z3, y)\n",
    "            \n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            if self.verbose:\n",
    "                print('loss : {}'.format(self.loss_train[i]))\n",
    "        \n",
    "            # 検証データ\n",
    "            if X_val is not None:\n",
    "                #\n",
    "                # 検証データの出力値を求める\n",
    "                #\n",
    "                A1 = self.Conv1d.forward(X)\n",
    "                A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.Affine2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.Affine3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                \n",
    "                # lossの計算\n",
    "                self.loss_val[i] = self.get_loss(Z3, y_val) \n",
    "            \n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        A1 = self.Conv1d.forward(X)\n",
    "        A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.Affine2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.Affine3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "\n",
    "        # 最も大きいインデックスをクラスとして採用\n",
    "        return np.argmax(Z3, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f545510-df82-4a6e-8119-1885606190be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## データ呼び出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e393748-9b0b-411a-8aab-d36475b4af18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ll/cdp3xh194t7_9c631lm07yx00000gn/T/xpython_36328/797439057.py:9: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_train = X_train.astype(np.float)\n",
      "/var/folders/ll/cdp3xh194t7_9c631lm07yx00000gn/T/xpython_36328/797439057.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_test = X_test.astype(np.float)\n"
     ]
    }
   ],
   "source": [
    "# データ読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 画像データ→行データに\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# 正規化\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# onehotベクトル化\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "# 訓練データと評価データに\n",
    "X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004ae207-04c6-4a38-bb1d-ad760c3a17af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## データテスト (問題8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "441bf189-7af7-4fcd-b48f-8938a3eb7d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.36654925226209994\n",
      "loss : 0.2100192137210512\n",
      "loss : 0.1476066566193128\n",
      "loss : 0.10816095624936085\n",
      "loss : 0.0793243116874951\n",
      "loss : 0.062415243723647026\n",
      "loss : 0.04582277916720315\n",
      "loss : 0.03435095643907364\n",
      "loss : 0.0252719218363128\n",
      "loss : 0.01911220550603863\n",
      "loss : 0.01536212708929432\n",
      "loss : 0.012334356182262614\n",
      "loss : 0.010196774264342975\n",
      "loss : 0.008730458040716505\n",
      "loss : 0.0075230554095277475\n",
      "loss : 0.006553425135048313\n",
      "loss : 0.005836565550246734\n",
      "loss : 0.00520012009225678\n",
      "loss : 0.004732264690962685\n",
      "loss : 0.004296952560076366\n"
     ]
    }
   ],
   "source": [
    "test_15 = ScratchCNNClassifier(n_nodes2=200, n_output=10, n_filiter=15)\n",
    "test_15.fit(X_train_[:1000], y_train_[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02e8e225-1437-4094-8a98-4062f36046ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9064"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = test_15.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bb1cff1-fcbe-4ef4-a670-554d353419c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.3696905397039646\n",
      "loss : 0.22442983085625318\n",
      "loss : 0.15832691966600895\n",
      "loss : 0.11433030418232379\n",
      "loss : 0.08593795324533252\n",
      "loss : 0.06411478326287486\n",
      "loss : 0.04761248055546649\n",
      "loss : 0.035515374560468826\n",
      "loss : 0.027258329527224356\n",
      "loss : 0.021138199557473456\n",
      "loss : 0.01668566226770709\n",
      "loss : 0.013721391954614161\n",
      "loss : 0.011761896373854379\n",
      "loss : 0.00971122448101144\n",
      "loss : 0.00839543506239267\n",
      "loss : 0.0073176298302739876\n",
      "loss : 0.00648883720279384\n",
      "loss : 0.005797389270067692\n",
      "loss : 0.005232232781815362\n",
      "loss : 0.0047654509646295595\n"
     ]
    }
   ],
   "source": [
    "test_2 = ScratchCNNClassifier(n_nodes2=200, n_output=10, n_filiter=2)\n",
    "test_2.fit(X_train_[:1000], y_train_[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aaf05f53-b425-4149-aa4d-86b61c867851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9054"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = test_2.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f2ff2e-90da-41ce-8b9d-c6707f0c156d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## -------------(2d)--------------\n",
    "## Conv2d (問題1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bbdf79-0242-4997-ac28-ca2ede41495d",
   "metadata": {},
   "source": [
    "順伝播\n",
    "![Imgur](https://i.imgur.com/yIVCm53h.jpg)\n",
    "\n",
    "逆伝播\n",
    "![Imgur](https://i.imgur.com/pztwHteh.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ae0261e9-ebdc-4c6f-81f9-c20de9b8f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d():\n",
    "    \"\"\"\n",
    "    チャンネル数１の畳み込み層\n",
    "    \n",
    "    param\n",
    "    --------------------\n",
    "    self.W_size : ndarray (W_size, )\n",
    "    フィルタサイズ(重みのサイズ？？？？)\n",
    "    self.padding : int\n",
    "    パディングのサイズ\n",
    "    self.storide : int\n",
    "    ストライドのサイズ\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, FH, FW, FN, n_input_ch, optimizer, initializer, PH=0, PW=0, SH=1, SW=1,):\n",
    "        \n",
    "        self.FH = FH\n",
    "        self.FW = FW\n",
    "        self.FN = FN\n",
    "        \n",
    "        # inputCHは初期化のために早めに必要\n",
    "        self.n_input_ch = n_input_ch\n",
    "        \n",
    "        self.PH = PH\n",
    "        self.PW = PW\n",
    "        self.SH = SH\n",
    "        self.SW = SW\n",
    "        \n",
    "    \n",
    "        self.W = initializer.init_W(FN, n_input_ch, FH, FW)\n",
    "        self.B = initializer.init_B(FN)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Adagradで使用する各層の前回までの重み\n",
    "        self.H_before_W = np.zeros_like(self.W)\n",
    "        self.H_before_B = np.zeros_like(self.B)\n",
    "\n",
    "        \n",
    "    def output_shape2d(self):\n",
    "        \"\"\"出力サイズ計算\n",
    "        H : 入力配列の高さ\n",
    "        W : 入力配列の横幅\n",
    "        FH : フィルターの高さ\n",
    "        FW : フィルターの横幅\n",
    "        PH : パディング数（縦）\n",
    "        PW : パディング数（横）\n",
    "        SH : ストライド数（縦）\n",
    "        SW : ストライド数（横）\n",
    "        \"\"\"\n",
    "        # 高さ計算\n",
    "        OH = (self.IH + 2 * self.PH - self.FH) / self.SH + 1\n",
    "        # 横幅計算\n",
    "        OW = (self.IW + 2 * self.PW - self.FW) / self.SW + 1\n",
    "        \n",
    "        return int(OH),int(OW)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \n",
    "        param\n",
    "        --------------\n",
    "        self.X : ndarray (n_features, )\n",
    "        入力サイズ\n",
    "        \"\"\" \n",
    "        \n",
    "        \n",
    "        # バッチ数, 入力高さ、　入力横幅を取得\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.IH = X.shape[2]\n",
    "        self.IW = X.shape[3]\n",
    "        \n",
    "        # Padiing実装\n",
    "        # pad(変更したいデータ, [(1次元目の上、　下), (2次元目の上、　下) ,(3次元目の上、　下), (4次元目の上、　下)])\n",
    "        # 3次元目と4次元目にPH,PWを足したい\n",
    "        X = np.pad(X, [(0, 0), (0, 0), (self.PH, self.PH), (self.PW, self.PW)])\n",
    "        self.X = X\n",
    "        \n",
    "        # 出力の高さ、横幅を取得\n",
    "        self.OH, self.OW = self.output_shape2d()\n",
    "        \n",
    "        # 出力の最終的な形の作成\n",
    "        A = np.zeros((self.n_samples, self.FN, self.OH, self.OW))\n",
    "        \n",
    "        \n",
    "        # 出力の計算\n",
    "        # サンプル数方向\n",
    "        for n in range(self.n_samples):\n",
    "            # ch方向\n",
    "            for FN in range(self.FN):\n",
    "                # 高さ方向\n",
    "                for OH in range(self.OH):\n",
    "                    # 横方向\n",
    "                    for OW in range(self.OW):\n",
    "                        \n",
    "                        # Xはch方向に足し合わせるので2次元方向の指定はいらないはず\n",
    "                        # 3, 4次元はOF, OW分スライドしていって、Aの正しい場所に入っていって欲しい\n",
    "                        # Wについては１次元(個数方向)の指定だけであとは足し合わせていくはず\n",
    "                        \n",
    "                        A[n, FN, OH, OW] = np.sum(X[n, :, OH:OH+self.FH, OW:OW+self.FW] * self.W[FN, :, :, :]) + self.B[FN]\n",
    "            \n",
    "        return A\n",
    "        \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \"\"\"\n",
    "\n",
    "        # 初期化\n",
    "        dX = np.zeros_like(self.X).astype(float)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        dB = np.zeros_like(self.B)\n",
    "\n",
    "        # 誤差の計算(重み方向)\n",
    "        # フィルタ数\n",
    "        for FN in range(self.FN):\n",
    "            # ch方向\n",
    "            for ch in range(self.n_input_ch):\n",
    "                # サンプル数方向\n",
    "                # ここが正直よくわからない。\n",
    "                # Wが個数に関係のない値をとるので、サンプル数毎の合計値になるはず\n",
    "                # 高さ方向\n",
    "                for FH in range(self.FH):\n",
    "                    # 横方向\n",
    "                    for FW in range(self.FW):\n",
    "                        dW[FN, ch, FH, FW] = np.sum(self.X[:, ch, FH:FH+self.OH, FW:FW+self.OW] * dA[:, FN, :, :])\n",
    "                            \n",
    "            # FH, FWの方向に合計\n",
    "            dB[FN] = np.sum(dA[:, FN, :, :])\n",
    "        \n",
    "        # 更新\n",
    "        self.dW = dW\n",
    "        self.dB = dB\n",
    "        self = self.optimizer.update(self)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # 誤差の計算(誤差方向)\n",
    "        # どうやってもおそらく[X-W]分paddingしないと次元を大きく復元できる気がしないのでwを差分paddingする。\n",
    "        # またch方向は足し合わせて良いはず、　平均な訳はないし\n",
    "        \n",
    "        # フィルタ数\n",
    "        for n in range(self.n_samples):\n",
    "            # ch方向\n",
    "            for FN in range(self.FN):\n",
    "                # 高さ方向\n",
    "                for OH in range(self.OH):\n",
    "                    # 横方向\n",
    "                    for OW in range(self.OW):\n",
    "                        # FN方向は合計値でいいはず\n",
    "                        # 各々を足し合わせる\n",
    "                        dX[n, :, OH:OH+self.FH, OW:OW+self.FW] += dA[n, FN, OH, OW] * self.W[FN, ch, :, :]\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "723a63be-bbc1-4796-b8bd-154f1a05711e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = Conv2d(FH=3, FW=3, FN=2, n_input_ch=1, optimizer=AdaGrad(lr=0.01), initializer=SimpleInitializer(sigma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2423804-7a29-402a-8e67-d472511f9b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 2, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[[[ 1,  2,  3,  4],\n",
    "                [ 5,  6,  7,  8],\n",
    "                [ 9, 10, 11, 12],\n",
    "                [13, 14, 15, 16]]]])\n",
    "\n",
    "w = np.array([[[[ 0.,  0.,  0.],\n",
    "               [ 0.,  1.,  0.],\n",
    "               [ 0., -1.,  0.]]],\n",
    "              [[[ 0.,  0.,  0.],\n",
    "               [ 0., -1.,  1.],\n",
    "               [ 0.,  0.,  0.]]]])\n",
    "\n",
    "test.W = w\n",
    "test.forward(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "661a9db2-5071-4d59-9944-29ff714e5707",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1, 4, 4), (2, 1, 3, 3))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba579e34-94b4-42e9-ac79-5511854f13bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "Diverの(?, 1, 2, 2)が正しいとするならば、したのDAは(2.1.2.2)になるが、そもそもそれだと上で求めたAとshapeが違う、、、  \n",
    "そうなると、X, w, bを貰わないと計算できない。  \n",
    "\n",
    "てかそもそも次の様な値って何(dW, dB, dX)のどれか。おそらく問題的にdXだと思うが。  \n",
    "その場合(2.2)の訳がない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffcdba35-d707-49c3-9543-a08f938642b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 2, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dA = np.array([[[[ -4,  -4],\n",
    "                   [ 10,  11]]],\n",
    "                  [[[  1,  -7],\n",
    "                   [  1, -11]]]])\n",
    "\n",
    "dA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64657651-b1f3-44eb-a76b-d6e8cf3b4ea8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 2, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多分これじゃないと解けない気がする\n",
    "dA = np.array([[[[ -4,  -4],\n",
    "                   [ 10,  11]],\n",
    "                  [[  1,  -7],\n",
    "                   [  1, -11]]]])\n",
    "\n",
    "dA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0977593b-37c3-4745-89c0-791e365e6648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 5.000e-02,  2.000e-02,  2.000e-02, -3.000e-02],\n",
       "         [-4.000e-02, -5.290e+00,  3.710e+00, -7.250e+00],\n",
       "         [-4.000e-02,  1.271e+01,  2.671e+01, -1.125e+01],\n",
       "         [-9.000e-02, -1.031e+01, -1.131e+01, -2.200e-01]]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.W = w\n",
    "test.backward(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be97dad2-c015-4488-bd27-40d6950fe04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[ 104.,  117.,  130.],\n",
       "          [ 156.,  169.,  182.],\n",
       "          [ 208.,  221.,  234.]]],\n",
       " \n",
       " \n",
       "        [[[ -74.,  -90., -106.],\n",
       "          [-138., -154., -170.],\n",
       "          [-202., -218., -234.]]]]),\n",
       " array([ 13., -16.]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# この二つの値は問題なし\n",
    "test.dW, test.dB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd1f1a0-5f4b-4ceb-84de-c8569ba0648d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## MaxPool2D (問題5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5d36bac-e0c5-478f-82e1-6f0efc9f63d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D():\n",
    "    \n",
    "    \"\"\"\n",
    "    最大プーリング層\n",
    "    \"\"\"\n",
    "    def __init__(self,PS):\n",
    "        \"\"\"コンストラクタ\n",
    "        Parameters\n",
    "        -----------\n",
    "        PU : プーリング幅(縦横同じ大きさ)\n",
    "        \"\"\"\n",
    "        self.PS = PS\n",
    "        # 順伝播の返り値\n",
    "        self.PA = None\n",
    "        # 最大値のインデックス記録\n",
    "        self.Pindex = None\n",
    "        \n",
    "    def forward(self,A):\n",
    "        \"\"\"順伝播\n",
    "        Parameters\n",
    "        -----------\n",
    "        A : 入力配列\n",
    "        \"\"\"\n",
    "        \n",
    "        #\n",
    "        self.A = A\n",
    "        # 入力配列のサイズ\n",
    "        N, CH, IH, IW = A.shape\n",
    "\n",
    "        PS = self.PS\n",
    "        \n",
    "        # 縦軸と横軸のスライド回数\n",
    "        PH, PW = int(IH/PS),int(IW/PS)\n",
    "        \n",
    "        # 各種パラメータの保存\n",
    "        # backwardで使うため、この様な形で保存\n",
    "        self.params = N, CH, IH, IW, PS, PH, PW\n",
    "        \n",
    "        # プーリング処理のための初期化\n",
    "        PA = np.zeros([N, CH, PH, PW])\n",
    "        Pindex = np.zeros([N, CH, PH, PW])\n",
    "        \n",
    "        # バッチ数でループ\n",
    "        for n in range(N):\n",
    "            # フィルター数でループ\n",
    "            for ch in range(CH):\n",
    "                # 縦方向スライド回数\n",
    "                for ph in range(PH):\n",
    "                    #ph方向スライド回数\n",
    "                    for pw in range(PW):\n",
    "                        # 順伝播の計算\n",
    "                        # プーリング開始位置をPS分スライドしていく\n",
    "                        PA[n, ch, ph, pw] = np.max(A[n, ch, ph*PS : ph*PS+PS, pw*PS : pw*PS+PS])\n",
    "                        \n",
    "                        # 最大値のインデックス記録\n",
    "                        Pindex[n, ch, ph, pw] = np.argmax(A[n, ch, ph*PS : ph*PS+PS, pw*PS : pw*PS+PS])\n",
    "                        \n",
    "        self.PA = PA\n",
    "        self.Pindex = Pindex\n",
    "                        \n",
    "        return PA\n",
    "    \n",
    "    def backward(self,dA):\n",
    "        \"\"\"逆伝播の値\n",
    "        Parameters\n",
    "        -----------\n",
    "        dA : 逆伝播してきた値\n",
    "        \"\"\"\n",
    "        # 保存しておいた各種パラメータ取得\n",
    "        N, CH, IH, IW, PS, PH, PW = self.params\n",
    "        \n",
    "        # 初期化\n",
    "        dP = np.zeros_like(self.A)\n",
    "        \n",
    "        # バッチ数でループ\n",
    "        for n in range(N):\n",
    "            # フィルター数でループ\n",
    "            for ch in range(CH):\n",
    "                # 縦方向スライド回数\n",
    "                for ph in range(PH):\n",
    "                    # 横方向スライド回数\n",
    "                    for pw in range(PW):\n",
    "                        # 最大値を取得してきたインデックスの取得\n",
    "                        idx = self.Pindex[n, ch, ph, pw]\n",
    "\n",
    "                        # プーリングされたあとの値の格納\n",
    "                        # 復元用\n",
    "                        tmp = np.zeros((PS*PS))\n",
    "\n",
    "                        for i in range(PS*PS):\n",
    "                            # 該当インデックスはその値\n",
    "                            if i == idx:\n",
    "                                tmp[i] = dA[n, ch, ph, pw]\n",
    "\n",
    "                            else:\n",
    "                                tmp[i] = 0\n",
    "\n",
    "                        # 返り値の該当場所に格納\n",
    "                        # 左上の2 * 2から順繰り埋めていく\n",
    "                        dP[n, ch, ph*PS : ph*PS+PS, pw*PS : pw*PS+PS] = tmp.reshape(PS, PS)\n",
    "        \n",
    "        return dP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df6ded3e-14c1-4698-ba2f-64477bc25175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------X\n",
      "[[[[5 1 7 2 1 0]\n",
      "   [1 0 1 1 6 8]\n",
      "   [8 3 2 0 7 4]\n",
      "   [6 0 5 3 3 4]\n",
      "   [0 6 5 3 0 0]\n",
      "   [1 2 0 2 2 6]]]]\n",
      "---------------A\n",
      "[[[[5. 7. 8.]\n",
      "   [8. 5. 7.]\n",
      "   [6. 5. 6.]]]]\n",
      "---------------dA\n",
      "[[[[5. 7. 8.]\n",
      "   [8. 5. 7.]\n",
      "   [6. 5. 6.]]]]\n",
      "---------------dZ\n",
      "[[[[5 0 7 0 0 0]\n",
      "   [0 0 0 0 0 8]\n",
      "   [8 0 0 0 7 0]\n",
      "   [0 0 5 0 0 0]\n",
      "   [0 6 5 0 0 0]\n",
      "   [0 0 0 0 0 6]]]]\n"
     ]
    }
   ],
   "source": [
    "# テスト\n",
    "# データ準備\n",
    "X = np.random.randint(0,9,36).reshape(1,1,6,6)\n",
    "print(\"---------------X\")\n",
    "print(X)\n",
    "\n",
    "# インスタンス生成と順伝播\n",
    "Pooling = MaxPool2D(PS=2)\n",
    "A = Pooling.forward(X)\n",
    "print(\"---------------A\")\n",
    "print(A)\n",
    "\n",
    "# 逆伝播してきた配列定義\n",
    "dA = A\n",
    "print(\"---------------dA\")\n",
    "print(dA)\n",
    "\n",
    "# 逆伝播\n",
    "dZ = Pooling.backward(dA)\n",
    "print(\"---------------dZ\")\n",
    "print(dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ac522e-1021-434c-b684-f7636250bf27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Flatten (問題6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d024490-46ee-4181-acbf-dcf4aa6633ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        インスタンス以外を一次元に変更\n",
    "        \"\"\"\n",
    "        \n",
    "        self.shape = X.shape \n",
    "        XX = X.reshape(X.shape[0], -1)\n",
    "        \n",
    "        return XX\n",
    "        \n",
    "    def backward(self, XX):\n",
    "        \n",
    "        X = XX.reshape(self.shape)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd917de8-9767-4447-b147-10fffcfe5073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 6, 6)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f8733d0-57e9-46e8-b106-40ddfa434310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 36)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Flatten()\n",
    "XX = test.forward(X)\n",
    "XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1855dffd-6252-47a8-9606-0febc82dabb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 6, 6)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.backward(XX).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e67962-09c4-4345-a0e3-149a608087af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ScratchCNNClassifier 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9451cbec-55e1-42f8-a591-1030086eb9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchCNNClassifier_2d:\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_nodes1, n_nodes2, n_output, epoch=5, lr=0.02, sigma=0.1, verbose = True, Activater=Tanh, Optimizer=AdaGrad):\n",
    "        \"\"\"\n",
    "        self.sigma : ガウス分布の標準偏差\n",
    "        self.lr : 学習率\n",
    "        self.n_nodes2 : 2層目のノード数\n",
    "        self.n_output : 出力層のノード数\n",
    "        self.epoch : エポック数\n",
    "        self.loss_train : 訓練データの損失\n",
    "        self.loss_val : 検証データの損失\n",
    "        self.verbose : 学習過程を表示するか\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "        self.lr = lr\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.epoch = epoch\n",
    "        self.loss_train = np.zeros(epoch)\n",
    "        self.loss_val = np.zeros(epoch)\n",
    "        self.verbose = verbose\n",
    "        self.Activater = Activater\n",
    "        if Activater == Sigmoid or Activater == Tanh:\n",
    "            self.Initializer = XavierInitializer\n",
    "        elif Activater == ReLU:\n",
    "            self.Initializer = HeInitializer\n",
    "        self.Optimizer = Optimizer\n",
    "        \n",
    "        \n",
    "    def get_loss(self, X, y_ture):\n",
    "        \"\"\"\n",
    "        クロスエントロピー誤差を計算\n",
    "        log(X)が最大値0の値を取るため-をかける必要がある。\n",
    "        全て正解の場合0を取る。\n",
    "\n",
    "        param\n",
    "        -------------------\n",
    "        X : 次の形のndarray(batch_size, n_features)\n",
    "        入力値\n",
    "        y_ture : 次の形のndarray(batch_size, n_class)\n",
    "        正解ラベル\n",
    "\n",
    "        return\n",
    "        --------------------\n",
    "        L : float(スカラー)\n",
    "        \"\"\"\n",
    "\n",
    "        h = 1e-7      \n",
    "        L = - np.sum(y_ture * np.log(X + h) / len(y_ture))\n",
    "        return L\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "        訓練データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        X = X[:, np.newaxis, :, :]\n",
    "        \n",
    "        # 1層目の入力特徴量数\n",
    "        self.n_features = X.shape[1]\n",
    "        \n",
    "        #\n",
    "        # フィルターの数はハイパーパラメータ\n",
    "        #\n",
    "        \n",
    "        # 1*28*28 → 10*26*26 Convなので初期化はシンプルなやつ\n",
    "        self.Conv2d = Conv2d(FH=3, FW=3, FN=10, n_input_ch=1 ,initializer=SimpleInitializer(self.sigma), optimizer=self.Optimizer(self.lr))\n",
    "        self.activation1 = ReLU()\n",
    "        # PSの初期値はわからん\n",
    "        # 10*26*26 → 10*13*13\n",
    "        self.MaxPool2D = MaxPool2D(PS=2)\n",
    "        # 10*13*13 → 1690\n",
    "        self.Flatten = Flatten()\n",
    "        \n",
    "        # 1690→700\n",
    "        self.Affine2 = Affine(n_nodes1=self.n_nodes1, n_nodes2=self.n_nodes2, initializer=self.Initializer(), optimizer=self.Optimizer(self.lr))\n",
    "        self.activation2 = ReLU()\n",
    "        # 700→10\n",
    "        self.Affine3 = Affine(n_nodes1=self.n_nodes2, n_nodes2=self.n_output, initializer=self.Initializer(), optimizer=self.Optimizer(self.lr))\n",
    "        self.activation3 = Softmax()      \n",
    "        \n",
    "        # エポック毎に更新\n",
    "        for i in range(self.epoch):\n",
    "            \n",
    "            # バッチ処理を実行\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=20)\n",
    "            \n",
    "            #\n",
    "            # バッチでループ\n",
    "            #\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "\n",
    "                # 順伝播\n",
    "                A1 = self.Conv2d.forward(mini_X_train)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                P1 = self.MaxPool2D.forward(Z1)\n",
    "                F1 = self.Flatten.forward(P1)\n",
    "                A2 = self.Affine2.forward(F1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.Affine3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "\n",
    "                # 逆伝播\n",
    "                dA3 = self.activation3.backward(mini_y_train) \n",
    "                dZ2 = self.Affine3.backward(dA3)\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dF1 = self.Affine2.backward(dA2)\n",
    "                dP1 = self.Flatten.backward(dF1)\n",
    "                dZ1 = self.MaxPool2D.backward(dP1)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.Conv2d.backward(dA1) \n",
    "                \n",
    "            #\n",
    "            # 更新パラメータを使って全データで検証\n",
    "            #\n",
    "            \n",
    "            # ch方向に軸を追加\n",
    "            A1 = self.Conv2d.forward(X)\n",
    "            Z1 = self.activation1.forward(A1)\n",
    "            P1 = self.MaxPool2D.forward(Z1)\n",
    "            F1 = self.Flatten.forward(P1)\n",
    "            A2 = self.Affine2.forward(F1)\n",
    "            Z2 = self.activation2.forward(A2)\n",
    "            A3 = self.Affine3.forward(Z2)\n",
    "            Z3 = self.activation3.forward(A3)\n",
    "            \n",
    "            # lossの計算\n",
    "            self.loss_train[i] = self.get_loss(Z3, y)\n",
    "            \n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            if self.verbose:\n",
    "                print('loss : {}'.format(self.loss_train[i]))\n",
    "        \n",
    "            # 検証データ\n",
    "            if X_val is not None:\n",
    "                #\n",
    "                # 検証データの出力値を求める\n",
    "                #\n",
    "                # ch方向に軸を追加\n",
    "                X_val = X_val[:, np.newaxis, :, :]\n",
    "                A1 = self.Conv2d.forward(X)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                P1 = self.MaxPool2D.forward(Z1)\n",
    "                F1 = self.Flatten.forward(P1)\n",
    "                A2 = self.Affine2.forward(F1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.Affine3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                \n",
    "                # lossの計算\n",
    "                self.loss_val[i] = self.get_loss(Z3, y_val) \n",
    "            \n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        X = X[:, np.newaxis, :, :]\n",
    "        A1 = self.Conv2d.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        P1 = self.MaxPool2D.forward(Z1)\n",
    "        F1 = self.Flatten.forward(P1)\n",
    "        A2 = self.Affine2.forward(F1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.Affine3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "\n",
    "        # 最も大きいインデックスをクラスとして採用\n",
    "        return np.argmax(Z3, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c64540d-7fe2-47f9-b8e3-4f163ae4807f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 実行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ba35a-0371-4d98-9c79-7f737dad59d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ff60d03-556a-4ee1-aa9e-123e5002a4ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(12000, 28, 28)\n",
      "(60000,)\n",
      "(12000, 10)\n"
     ]
    }
   ],
   "source": [
    "# データ読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 正規化\n",
    "X_train = X_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# onehotベクトル化\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "# 訓練データと評価データに\n",
    "X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db84293d-9d6d-4034-911a-99ecfaa1f4db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## データテスト(問題7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c702aeaf-d1f2-4290-88f0-d5b9453f4966",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ScratchCNNClassifier_2d(n_nodes1=1690, n_nodes2=700, n_output=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "bb9de2cc-6685-4a53-a7e2-b3bf13b9ab17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.9199656985771756\n",
      "loss : 0.30569435536876155\n",
      "loss : 0.11531611048662557\n",
      "loss : 0.06290123838119972\n",
      "loss : 0.035144826222562923\n"
     ]
    }
   ],
   "source": [
    "test.fit(X_train_[:200], y_train_[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "fd5c42e6-2781-4135-b5d5-0ac1b501d52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = test.predict(X_test[:100])\n",
    "accuracy_score(y_test[:100], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29055ec5-1235-4fbe-b9a0-bb7994d4c41c",
   "metadata": {},
   "source": [
    "実行に時間がかかるので、サンプル数とエポックを相当削っているがそれでもまぁまぁの性能を示した。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052af98f-deaf-4747-ad84-3f05d2684650",
   "metadata": {},
   "source": [
    "# 【問題10】出力サイズとパラメータ数の計算\n",
    "CNNモデルを構築する際には、全結合層に入力する段階で特徴量がいくつになっているかを事前に計算する必要があります。\n",
    "\n",
    "\n",
    "また、巨大なモデルを扱うようになると、メモリや計算速度の関係でパラメータ数の計算は必須になってきます。フレームワークでは各層のパラメータ数を表示させることが可能ですが、意味を理解していなくては適切な調整が行えません。\n",
    "\n",
    "\n",
    "以下の3つの畳み込み層の出力サイズとパラメータ数を計算してください。パラメータ数についてはバイアス項も考えてください。\n",
    "\n",
    "\n",
    "1.\n",
    "\n",
    "入力サイズ : 144×144, 3チャンネル\n",
    "フィルタサイズ : 3×3, 6チャンネル\n",
    "ストライド : 1\n",
    "パディング : なし\n",
    "\n",
    "OUT : 6 * 142 * 142  \n",
    "W : 6 * 3 * 3 * 3  \n",
    "B : 6 \n",
    "\n",
    "2.\n",
    "\n",
    "入力サイズ : 60×60, 24チャンネル\n",
    "フィルタサイズ : 3×3, 48チャンネル\n",
    "ストライド　: 1\n",
    "パディング : なし\n",
    "\n",
    "OUT : 48 * 58 * 58  \n",
    "W : 48 * 24 * 3 * 3  \n",
    "B : 48  \n",
    "\n",
    "3.\n",
    "\n",
    "入力サイズ : 20×20, 10チャンネル\n",
    "フィルタサイズ: 3×3, 20チャンネル\n",
    "ストライド : 2\n",
    "パディング : なし\n",
    "\n",
    "OUT : 20 * 9 * 9  \n",
    "W : 20 * 10 * 3 * 3  \n",
    "B : 20  \n",
    "\n",
    "＊最後の例は丁度良く畳み込みをすることができない場合です。フレームワークでは余ったピクセルを見ないという処理が行われることがあるので、その場合を考えて計算してください。端が欠けてしまうので、こういった設定は好ましくないという例です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09c4728-6ca5-4ad3-aadf-4315e4901b85",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "# {雑談}  for文の文字を同じにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8753e036-0eeb-4cf8-bb23-100ba836a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "PH = 3\n",
    "PW = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b50e294-7ffe-4588-bc45-b2e302130db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PH : 0, PW : 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PH : 0, PW : 1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PH : 0, PW : 2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PH : 1, PW : 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PH : 1, PW : 1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PH : 2, PW : 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for PH in range (PH):\n",
    "    for PW in range (PW):\n",
    "        display('PH : {}, PW : {}'.format(PH, PW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f76e8852-5daf-45c5-bf88-3419e4dcd961",
   "metadata": {},
   "outputs": [],
   "source": [
    "PH = 3\n",
    "PW = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aac0d520-ee91-493f-944f-8172692a027a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PH : 0, PW : 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PH : 0, PW : 1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PH : 0, PW : 2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PH : 1, PW : 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PH : 1, PW : 1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PH : 1, PW : 2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PH : 2, PW : 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PH : 2, PW : 1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PH : 2, PW : 2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range (PH):\n",
    "    for j in range (PW):\n",
    "        display('PH : {}, PW : {}'.format(i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b88347d-4c36-4015-9a01-028f2f48985a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
